from langchain_core.messages import AIMessage
from configuration import state
from prompts import graph_and_exploitation_inference_prompt
from .node_utils import OPEN_AI_KEY, get_last_epoch_fields, DeltaOutput, merge_deltas_into_graph, enforce_backfill_on_deltas
from openai import BadRequestError 
import logging
import json
import instructor
from openai import OpenAI
import copy
from pydantic import ValidationError

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


async def graph_and_exploitation_inference(state: state.AgentState, config):
    """
    Infers/Update the attack graph from security events
    """
    logger.info("Inference Agent")
    episodic_memory = config.get("configurable", {}).get("store")
    model_name = config.get("configurable", {}).get("model_config", "large:4.1")

    last_epoch = episodic_memory.get_recent_iterations(limit=1)
    last_exploitation, last_attack_graph = get_last_epoch_fields(last_epoch)
    
    if not last_attack_graph or "edges" not in last_attack_graph:
        logger.info("Initializing first-epoch attack graph baseline")
        last_attack_graph = {"edges": [], "interesting": []}

    if not isinstance(last_exploitation, list) or len(last_exploitation) == 0:
        logger.info("Initializing first-epoch containers exploitation baseline")
        last_exploitation = [
            {
                "ip": h["ip"],
                "service": h["service"],
                "level_prev": 0,
                "level_new": 0,
                "changed": False,
                "evidence_quotes": []
            }
            for h in state.vulnerable_containers
        ]


    logger.info(f"Using: {model_name}")
    current_graph = copy.deepcopy(last_attack_graph)
    current_exploitation = copy.deepcopy(last_exploitation)

    message_lines = []

    try:
    
        for container in state.vulnerable_containers:
            container_ip = container.get("ip")
            container_service = container.get("service")
            logger.info(f"Processing container {container_ip} / {container_service}")

            events_for_container = []
            for ev in state.security_events:
                ev_ip = ev.get("ip")
                ev_service = ev.get("service")
                if (ev_ip is not None and ev_ip == container_ip) or (ev_service is not None and ev_service == container_service):
                    events_for_container.append(ev)

            if not events_for_container:
                logger.info(f"No security events for container {container_ip}; skipping LLM call")
                continue
            messages = [
                {"role" : "system", "content" : graph_and_exploitation_inference_prompt.SYSTEM_PROMPT},
                {"role" : "user", "content": graph_and_exploitation_inference_prompt.USER_PROMPT.substitute(
                    security_events=json.dumps(events_for_container, ensure_ascii=False, indent=2),
                    vulnerable_containers=container,
                    previous_attack_graph=json.dumps(current_graph, ensure_ascii=False, indent=2)
                )}
            ]

            logger.info(f"Calling LLM for container {container_ip}")
            try:
                
                agent = instructor.from_openai(OpenAI(api_key=OPEN_AI_KEY))
                response: DeltaOutput = agent.chat.completions.create(
                    model=model_name,
                    response_model=DeltaOutput,
                    temperature=0.2,
                    messages=messages # type: ignore
                )
                response = enforce_backfill_on_deltas(response, current_graph)

                merged = merge_deltas_into_graph(
                    prev_graph=current_graph,
                    prev_exploitation=current_exploitation,
                    vulnerable_containers=state.vulnerable_containers,
                    deltas=response
                )

                current_graph = merged["inferred_attack_graph"]
                current_exploitation = merged["containers_exploitation"]
                message_lines.append(f"[{container_ip}] LLM reasoning: {response.reasoning}")
            except BadRequestError as e:
                logger.error(f"Error: {e}")
            except Exception as e:
                logger.error(f"Error parsing json in attack graph inference:\n{e}")
            

        final_text = []
        final_text.extend(message_lines)
        final_text.append(f"Inferred Attack Graph: {str(current_graph)}")
        final_text.append(f"Containers' Exploitaiton: {str(current_exploitation)}")
        message = AIMessage(content="\n".join(final_text))


        return {
            "messages": [message],
            "inferred_attack_graph": current_graph, 
            "containers_exploitation": current_exploitation
        }
    except Exception as e:
        logger.error(f"Error in per-container attack graph inference: {e}")
        return {
            "messages" : [AIMessage(content="; ".join(message_lines) or "error during inference")]
        }